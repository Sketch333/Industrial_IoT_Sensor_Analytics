{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4yqNWZHxryZs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.signal import savgol_filter\n",
        "from scipy.stats import skew, entropy\n",
        "from scipy.fft import rfft\n",
        "import pywt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "# Optional: use fastdtw if installed, otherwise fallback to a simple approximate distance\n",
        "try:\n",
        "    from fastdtw import fastdtw\n",
        "    from scipy.spatial.distance import euclidean\n",
        "    has_fastdtw = True\n",
        "except Exception:\n",
        "    has_fastdtw = False\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"sensor_data.csv\"\n",
        "OUTPUT_DIR = \"output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def load_data(path=DATA_PATH):\n",
        "    df = pd.read_csv(path, parse_dates=[\"timestamp\"])\n",
        "    # unify column names if necessary\n",
        "    expected = [\"timestamp\", \"device_id\", \"temperature_c\", \"humidity_pct\",\n",
        "                \"vibration_level\", \"pressure_kpa\", \"light_lux\", \"gas_ppm\", \"anomaly_label\"]\n",
        "    # if columns slightly differ, try to normalize (user's file earlier had these names)\n",
        "    df = df.rename(columns={c: c.strip() for c in df.columns})\n",
        "    # sort\n",
        "    df = df.sort_values([\"device_id\", \"timestamp\"]).reset_index(drop=True)\n",
        "    return df"
      ],
      "metadata": {
        "id": "pjJOYHSdumeB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Time-Series Integrity & Cleaning\n",
        "Objectives\n",
        "Validate time frequency, identify missing intervals, and impute multivariate sensor values.\n",
        "Requirements\n",
        "1. Verify that each device reports data every 30 seconds.\n",
        "2. Identify devices with gaps longer than 5 minutes and count missing intervals.\n",
        "3. Reconstruct missing timestamps using reindexing.\n",
        "4. Impute:\n",
        "• Temperature: linear interpolation\n",
        "• Humidity: rolling median\n",
        "• Vibration: exponential weighted mean\n",
        "• Pressure/Light/Gas: student chooses appropriate techniques\n",
        "Deliverables\n",
        "Table of missing timestamps per device and a cleaned DataFrame."
      ],
      "metadata": {
        "id": "pU4pV-uzsVLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def task1_integrity_and_cleaning(df, freq_seconds=30, report_csv=True):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      missing_df: per-device missing summary\n",
        "      cleaned_df: reindexed and imputed DataFrame\n",
        "    \"\"\"\n",
        "    freq = pd.Timedelta(seconds=freq_seconds)\n",
        "    missing_info = []\n",
        "\n",
        "    # ensure timestamp dtype\n",
        "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
        "\n",
        "    for device, g in df.groupby(\"device_id\"):\n",
        "        diffs = g[\"timestamp\"].diff()\n",
        "        # gaps bigger than expected\n",
        "        gaps = diffs[diffs > freq]\n",
        "        total_missing = sum(int(gap / freq) - 1 for gap in gaps)\n",
        "        # count gaps specifically over 5 minutes\n",
        "        gaps_over_5 = (diffs > pd.Timedelta(minutes=5)).sum()\n",
        "        missing_info.append([device, int(gaps_over_5), int(total_missing)])\n",
        "\n",
        "    missing_df = pd.DataFrame(missing_info, columns=[\"device_id\", \"num_gaps_over_5min\", \"missing_intervals\"])\n",
        "    missing_df = missing_df.sort_values(\"missing_intervals\", ascending=False).reset_index(drop=True)\n",
        "    if report_csv:\n",
        "        missing_df.to_csv(os.path.join(OUTPUT_DIR, \"missing_summary_per_device.csv\"), index=False)\n",
        "\n",
        "    # Reconstruct and impute\n",
        "    cleaned_devices = []\n",
        "    for device, g in df.groupby(\"device_id\"):\n",
        "        g = g.set_index(\"timestamp\").sort_index()\n",
        "        full_index = pd.date_range(start=g.index.min(), end=g.index.max(), freq=f\"{freq_seconds}s\")\n",
        "        g = g.reindex(full_index)\n",
        "        g[\"device_id\"] = device\n",
        "\n",
        "        # Impute rules\n",
        "        # Temperature: linear interpolation\n",
        "        if \"temperature_c\" in g.columns:\n",
        "            g[\"temperature_c\"] = g[\"temperature_c\"].interpolate(method=\"linear\")\n",
        "\n",
        "        # Humidity: rolling median (window = 5 samples ~ 2.5 minutes)\n",
        "        if \"humidity_pct\" in g.columns:\n",
        "            g[\"humidity_pct\"] = g[\"humidity_pct\"].fillna(\n",
        "                g[\"humidity_pct\"].rolling(window=5, center=True, min_periods=1).median()\n",
        "            )\n",
        "\n",
        "        # Vibration: exponential weighted mean (denoised baseline)\n",
        "        if \"vibration_level\" in g.columns:\n",
        "            g[\"vibration_level_raw\"] = g[\"vibration_level\"]  # keep a raw copy\n",
        "            g[\"vibration_level\"] = g[\"vibration_level\"].ewm(span=10, min_periods=1).mean()\n",
        "\n",
        "        # Pressure: time interpolation (linear in time)\n",
        "        if \"pressure_kpa\" in g.columns:\n",
        "            g[\"pressure_kpa\"] = g[\"pressure_kpa\"].interpolate(method=\"time\")\n",
        "\n",
        "        # Light: forward fill (blackouts expected -> 0), if still missing -> 0\n",
        "        if \"light_lux\" in g.columns:\n",
        "            g[\"light_lux\"] = g[\"light_lux\"].fillna(method=\"ffill\").fillna(0)\n",
        "\n",
        "        # Gas: linear interpolation\n",
        "        if \"gas_ppm\" in g.columns:\n",
        "            g[\"gas_ppm\"] = g[\"gas_ppm\"].interpolate(method=\"linear\")\n",
        "\n",
        "        # anomaly_label: keep original where present; optional fill\n",
        "        if \"anomaly_label\" in g.columns:\n",
        "            # leave as-is: interpolation doesn't make sense for labels\n",
        "            pass\n",
        "\n",
        "        cleaned_devices.append(g)\n",
        "\n",
        "    cleaned_df = pd.concat(cleaned_devices).reset_index().rename(columns={\"index\": \"timestamp\"})\n",
        "    # Save cleaned\n",
        "    cleaned_df.to_csv(os.path.join(OUTPUT_DIR, \"cleaned_sensor_data.csv\"), index=False)\n",
        "    return missing_df, cleaned_df"
      ],
      "metadata": {
        "id": "KLpkEHJNtoj-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Noise Reduction & Signal Decomposition\n",
        "Objectives\n",
        "Denoise signals and analyze underlying trends/seasonality.\n",
        "\n",
        "2\n",
        "\n",
        "Requirements\n",
        "1. Apply Savitzky–Golay smoothing to temperature.\n",
        "2. Apply exponential moving average to humidity.\n",
        "3. Perform wavelet denoising on vibration.\n",
        "4. Use seasonal decompose() to extract trend, seasonal, and residual components\n",
        "for each sensor variable.\n",
        "\n",
        "Deliverables\n",
        "Before–after plots and a summary of which smoothing technique performed best."
      ],
      "metadata": {
        "id": "cfcS_kwjvf4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def task2_denoise_and_decompose(cleaned_df, device=None, plot=True):\n",
        "    \"\"\"\n",
        "    Apply smoothing/denoising and seasonal decompose for sensors.\n",
        "    If device is None, runs an example on the first device.\n",
        "    Returns a dict of denoised series and decomposition results per variable.\n",
        "    \"\"\"\n",
        "    sensors = [\"temperature_c\", \"humidity_pct\", \"vibration_level\", \"pressure_kpa\", \"light_lux\", \"gas_ppm\"]\n",
        "    result = {}\n",
        "\n",
        "    # choose a device sample\n",
        "    if device is None:\n",
        "        device = cleaned_df[\"device_id\"].unique()[0]\n",
        "\n",
        "    df_dev = cleaned_df[cleaned_df[\"device_id\"] == device].set_index(\"timestamp\").sort_index()\n",
        "\n",
        "    # 1) Savitzky–Golay on temperature\n",
        "    temp = df_dev[\"temperature_c\"].copy().dropna()\n",
        "    window_length = 11 if len(temp) >= 11 else (len(temp) // 2) * 2 + 1\n",
        "    if window_length < 3:\n",
        "        temp_sg = temp\n",
        "    else:\n",
        "        temp_sg = pd.Series(savgol_filter(temp.values, window_length=window_length, polyorder=2), index=temp.index)\n",
        "\n",
        "    # 2) Exponential moving average on humidity\n",
        "    hum = df_dev[\"humidity_pct\"].copy()\n",
        "    hum_ema = hum.ewm(span=10, adjust=False).mean()\n",
        "\n",
        "    # 3) Wavelet denoising on vibration\n",
        "    vib = df_dev[\"vibration_level\"].dropna()\n",
        "    if len(vib) > 10:\n",
        "        # pywt denoising: use 'db4' and universal thresholding\n",
        "        coeffs = pywt.wavedec(vib.values, \"db4\", level=3)\n",
        "        sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n",
        "        uthresh = sigma * np.sqrt(2 * np.log(len(vib)))\n",
        "        denoised_coeffs = [pywt.threshold(c, value=uthresh, mode=\"soft\") for c in coeffs]\n",
        "        vib_denoised_vals = pywt.waverec(denoised_coeffs, \"db4\")\n",
        "        vib_denoised = pd.Series(vib_denoised_vals[:len(vib)], index=vib.index)\n",
        "    else:\n",
        "        vib_denoised = vib\n",
        "\n",
        "    # 4) Seasonal decomposition for each sensor variable (additive)\n",
        "    decompositions = {}\n",
        "    for s in sensors:\n",
        "        series = df_dev[s].dropna()\n",
        "        if len(series) > 24:  # enough data for decomposition\n",
        "            # infer freq in samples per day: if 30s samples then 2880 per day -> that's big.\n",
        "            # seasonal_decompose requires a period; choose a periodCandidate like daily or hourly based on length.\n",
        "            # We'll try a day-period if length allows, otherwise fallback to 24-hourly period converted to samples.\n",
        "            try:\n",
        "                period = None\n",
        "                # infer: if sample rate is 30s -> samples per day = 24*60*60/30 = 2880\n",
        "                # but seasonal_decompose may choke on large periods; use smaller period if data shorter.\n",
        "                samples_per_second = 1 / np.median(np.diff(series.index.astype(np.int64) // 10**9))\n",
        "                # not strictly necessary — choose period=2880 if enough length else 1440 or 288\n",
        "                if len(series) > 2880 * 2:\n",
        "                    period = 2880\n",
        "                elif len(series) > 1440:\n",
        "                    period = 1440\n",
        "                elif len(series) > 288:\n",
        "                    period = 288\n",
        "                else:\n",
        "                    period = None\n",
        "                if period:\n",
        "                    dec = seasonal_decompose(series, period=period, model=\"additive\", extrapolate_trend=\"freq\")\n",
        "                    decompositions[s] = dec\n",
        "                else:\n",
        "                    decompositions[s] = None\n",
        "            except Exception:\n",
        "                decompositions[s] = None\n",
        "        else:\n",
        "            decompositions[s] = None\n",
        "\n",
        "    result[\"temperature_sg\"] = temp_sg\n",
        "    result[\"humidity_ema\"] = hum_ema\n",
        "    result[\"vibration_wavelet\"] = vib_denoised\n",
        "    result[\"decompositions\"] = decompositions\n",
        "\n",
        "    if plot:\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        plt.plot(df_dev[\"temperature_c\"], alpha=0.4, label=\"raw temp\")\n",
        "        plt.plot(temp_sg, label=\"Savitzky-Golay\")\n",
        "        plt.title(f\"Temperature smoothing for {device}\")\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, f\"{device}_temperature_smoothing.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        plt.plot(df_dev[\"humidity_pct\"], alpha=0.4, label=\"raw humidity\")\n",
        "        plt.plot(hum_ema, label=\"EMA humidity\")\n",
        "        plt.title(f\"Humidity smoothing for {device}\")\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, f\"{device}_humidity_ema.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        plt.plot(df_dev[\"vibration_level\"], alpha=0.4, label=\"raw vib\")\n",
        "        plt.plot(vib_denoised, label=\"wavelet denoised\")\n",
        "        plt.title(f\"Vibration denoising for {device}\")\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, f\"{device}_vibration_wavelet.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    # return results for downstream\n",
        "    return result"
      ],
      "metadata": {
        "id": "k9MeB5BnvDrQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Device Behavior Profiling & Clustering\n",
        "Objectives\n",
        "Create behavioral fingerprints for each device and cluster them.\n",
        "Requirements\n",
        "1. Compute per-device summary statistics: mean, variance, quantiles, slopes, and\n",
        "threshold exceedance proportions.\n",
        "2. Construct a feature matrix for all 100 devices.\n",
        "3. Determine optimal clusters using K-means and silhouette score.\n",
        "4. Visualize clusters via PCA (2D).\n",
        "Deliverables\n",
        "Cluster assignments and a PCA scatterplot of device behavior profiles."
      ],
      "metadata": {
        "id": "g2VLDztXvk8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def task3_profile_and_cluster(cleaned_df, n_clusters_range=(2, 8), random_state=42, save=True):\n",
        "    devices = cleaned_df[\"device_id\"].unique()\n",
        "    feature_rows = []\n",
        "\n",
        "    def slope(series):\n",
        "        if series.isnull().all() or len(series.dropna()) < 2:\n",
        "            return 0.0\n",
        "        x = np.arange(len(series))\n",
        "        y = series.fillna(method=\"ffill\").fillna(method=\"bfill\").values\n",
        "        m, b = np.polyfit(x, y, 1)\n",
        "        return m\n",
        "\n",
        "    for device in devices:\n",
        "        d = cleaned_df[cleaned_df[\"device_id\"] == device].set_index(\"timestamp\")\n",
        "        row = {\"device_id\": device}\n",
        "        for col in [\"temperature_c\", \"humidity_pct\", \"vibration_level\", \"pressure_kpa\", \"light_lux\", \"gas_ppm\"]:\n",
        "            s = d[col].dropna()\n",
        "            row[f\"{col}_mean\"] = s.mean() if len(s) else np.nan\n",
        "            row[f\"{col}_var\"] = s.var() if len(s) else np.nan\n",
        "            row[f\"{col}_q25\"] = s.quantile(0.25) if len(s) else np.nan\n",
        "            row[f\"{col}_q50\"] = s.quantile(0.50) if len(s) else np.nan\n",
        "            row[f\"{col}_q75\"] = s.quantile(0.75) if len(s) else np.nan\n",
        "            row[f\"{col}_slope\"] = slope(s)\n",
        "            # threshold exceedance proportion: > mean + 2*std\n",
        "            if len(s) > 0:\n",
        "                th = s.mean() + 2 * s.std()\n",
        "                row[f\"{col}_th_exceed_prop\"] = (s > th).mean()\n",
        "            else:\n",
        "                row[f\"{col}_th_exceed_prop\"] = np.nan\n",
        "        feature_rows.append(row)\n",
        "\n",
        "    features_df = pd.DataFrame(feature_rows).set_index(\"device_id\")\n",
        "\n",
        "    # Fill NAs with column medians\n",
        "    features_df = features_df.fillna(features_df.median())\n",
        "\n",
        "    # Scale\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(features_df)\n",
        "\n",
        "    # Choose k by silhouette score\n",
        "    best_k = None\n",
        "    best_score = -1\n",
        "    best_labels = None\n",
        "    for k in range(n_clusters_range[0], n_clusters_range[1] + 1):\n",
        "        km = KMeans(n_clusters=k, random_state=random_state, n_init=10).fit(X)\n",
        "        labels = km.labels_\n",
        "        sc = silhouette_score(X, labels) if len(np.unique(labels)) > 1 else -1\n",
        "        if sc > best_score:\n",
        "            best_score = sc\n",
        "            best_k = k\n",
        "            best_labels = labels\n",
        "            best_km = km\n",
        "\n",
        "    features_df[\"cluster\"] = best_labels\n",
        "\n",
        "    # PCA for visualization\n",
        "    pca = PCA(n_components=2, random_state=random_state)\n",
        "    pcs = pca.fit_transform(X)\n",
        "    features_df[\"pc1\"] = pcs[:, 0]\n",
        "    features_df[\"pc2\"] = pcs[:, 1]\n",
        "\n",
        "    if save:\n",
        "        features_df.to_csv(os.path.join(OUTPUT_DIR, \"device_profiles_clusters.csv\"))\n",
        "        plt.figure(figsize=(10, 7))\n",
        "        sns.scatterplot(data=features_df, x=\"pc1\", y=\"pc2\", hue=\"cluster\", palette=\"tab10\", s=100)\n",
        "        for i, d in enumerate(features_df.index):\n",
        "            plt.text(features_df.loc[d, \"pc1\"] + 0.01, features_df.loc[d, \"pc2\"] + 0.01, d, fontsize=8)\n",
        "        plt.title(f\"Device clustering (k={best_k}, silhouette={best_score:.3f})\")\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, \"device_cluster_pca.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    return features_df, best_k, best_score"
      ],
      "metadata": {
        "id": "xthLnJUWvQF_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4: Rolling Windows & Advanced Feature Ex-\n",
        "traction\n",
        "\n",
        "Objectives\n",
        "Capture local behavior changes in sensors.\n",
        "\n",
        "3\n",
        "\n",
        "Requirements\n",
        "1. Compute rolling features for 1, 5, and 10-minute windows:\n",
        "• Mean\n",
        "• Standard deviation\n",
        "• Skewness\n",
        "• Entropy\n",
        "• FFT energy\n",
        "2. Detect local spikes or drops in sensor behavior.\n",
        "Deliverables\n",
        "A feature table and visualizations of rolling entropy for a selected device."
      ],
      "metadata": {
        "id": "Tph7EoOhvwjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_entropy(arr, base=2):\n",
        "    # normalize histogram\n",
        "    arr = np.array(arr)\n",
        "    if len(arr) == 0:\n",
        "        return 0.0\n",
        "    hist, _ = np.histogram(arr, bins=20, density=True)\n",
        "    hist = hist[hist > 0]\n",
        "    return entropy(hist, base=base)\n",
        "\n",
        "def task4_rolling_features(cleaned_df, device, windows_seconds=(60, 300, 600), freq_seconds=30):\n",
        "    \"\"\"\n",
        "    windows_seconds: windows in seconds e.g., 60,300,600 -> 1,5,10 minutes\n",
        "    returns feature_df with hierarchical column names: sensor_window_feature\n",
        "    \"\"\"\n",
        "    df_dev = cleaned_df[cleaned_df[\"device_id\"] == device].set_index(\"timestamp\").sort_index()\n",
        "    sensors = [\"temperature_c\", \"humidity_pct\", \"vibration_level\", \"pressure_kpa\", \"light_lux\", \"gas_ppm\"]\n",
        "    feature_frames = []\n",
        "\n",
        "    for win_s in windows_seconds:\n",
        "        win_samples = int(win_s / freq_seconds)\n",
        "        # rolling window\n",
        "        roll = df_dev[sensors].rolling(window=win_samples, min_periods=1, center=False)\n",
        "        feat = pd.DataFrame(index=df_dev.index)\n",
        "        for s in sensors:\n",
        "            feat[f\"{s}_mean_{win_s}s\"] = roll[s].mean()\n",
        "            feat[f\"{s}_std_{win_s}s\"] = roll[s].std().fillna(0)\n",
        "            feat[f\"{s}_skew_{win_s}s\"] = roll[s].apply(lambda x: skew(x.dropna()) if len(x.dropna()) > 2 else 0.0)\n",
        "            feat[f\"{s}_entropy_{win_s}s\"] = roll[s].apply(lambda x: compute_entropy(x.dropna()) if len(x.dropna()) > 2 else 0.0)\n",
        "            # FFT energy: compute energy of rfft magnitudes in the window\n",
        "            def fft_energy(x):\n",
        "                x = np.array(x.dropna())\n",
        "                if len(x) == 0:\n",
        "                    return 0.0\n",
        "                spec = np.abs(rfft(x))\n",
        "                return np.sum(spec ** 2) / len(spec)\n",
        "            feat[f\"{s}_fft_energy_{win_s}s\"] = roll[s].apply(fft_energy)\n",
        "\n",
        "        feature_frames.append(feat)\n",
        "\n",
        "    features_concat = pd.concat(feature_frames, axis=1)\n",
        "    features_concat.to_csv(os.path.join(OUTPUT_DIR, f\"{device}_rolling_features.csv\"))\n",
        "    return features_concat"
      ],
      "metadata": {
        "id": "Dp74RkbLvr9P"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5: Multivariate Correlation & Cross-Device Syn-\n",
        "chronization\n",
        "\n",
        "Objectives\n",
        "Study relationships between sensors and alignments across devices.\n",
        "Requirements\n",
        "1. Compute Pearson and Spearman correlations for all sensor pairs.\n",
        "2. Compute cross-correlation between at least three device pairs.\n",
        "3. Identify synchronized or environmentally coupled devices.\n",
        "Deliverables\n",
        "Correlation heatmap and analysis of synchronized device groups."
      ],
      "metadata": {
        "id": "THI0Y_-3v2EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def task5_correlations(cleaned_df, sample_fraction=0.1, save=True):\n",
        "    sensors = [\"temperature_c\", \"humidity_pct\", \"vibration_level\", \"pressure_kpa\", \"light_lux\", \"gas_ppm\"]\n",
        "    # compute correlations by pooling all devices (or aggregated)\n",
        "    pivot = cleaned_df.set_index(\"timestamp\").groupby(\"device_id\")[sensors].mean().T\n",
        "    # Pearson & Spearman across sensors pooled by device-means\n",
        "    pear = pivot.corr(method=\"pearson\")\n",
        "    spear = pivot.corr(method=\"spearman\")\n",
        "\n",
        "    if save:\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(pear, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
        "        plt.title(\"Pearson correlation between sensors (device-mean aggregated)\")\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, \"sensor_correlation_pearson.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(spear, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
        "        plt.title(\"Spearman correlation between sensors (device-mean aggregated)\")\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, \"sensor_correlation_spearman.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    # Cross-correlation between device pairs: pick three pairs (first 3 devices)\n",
        "    devices = cleaned_df[\"device_id\"].unique()[:6]\n",
        "    xc_results = {}\n",
        "    for i in range(0, min(3, len(devices)-1)):\n",
        "        d1 = devices[i]\n",
        "        d2 = devices[i+1]\n",
        "        # take temperature series, align timestamps by reindexing to common index\n",
        "        s1 = cleaned_df[cleaned_df[\"device_id\"] == d1].set_index(\"timestamp\")[\"temperature_c\"]\n",
        "        s2 = cleaned_df[cleaned_df[\"device_id\"] == d2].set_index(\"timestamp\")[\"temperature_c\"]\n",
        "        common_index = s1.dropna().index.intersection(s2.dropna().index)\n",
        "        s1a = s1.reindex(common_index).fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "        s2a = s2.reindex(common_index).fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "        # compute cross-correlation (normalized)\n",
        "        corr = np.correlate((s1a - s1a.mean())/s1a.std(), (s2a - s2a.mean())/s2a.std(), mode=\"full\")\n",
        "        lags = np.arange(-len(s1a)+1, len(s1a))\n",
        "        max_idx = np.argmax(corr)\n",
        "        lag_at_max = lags[max_idx]\n",
        "        xc_results[f\"{d1}_vs_{d2}\"] = {\"lag\": int(lag_at_max), \"max_corr\": float(corr[max_idx])}\n",
        "    return pear, spear, xc_results"
      ],
      "metadata": {
        "id": "vz7NnkE7v1SI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 6: Multi-Method Anomaly Detection\n",
        "Objectives\n",
        "Detect anomalies using at least three distinct approaches.\n",
        "\n",
        "4\n",
        "\n",
        "Methods Allowed\n",
        "- Statistical: Z-score, IQR\n",
        "-  Machine Learning: Isolation Forest, Local Outlier Factor\n",
        "- Probabilistic: Gaussian Mixture anomaly probabilities\n",
        "- Sequence-Based (optional advanced): Autoencoder or LSTM prediction error\n",
        "\n",
        "Deliverables\n",
        "Anomaly labels, precision/recall comparison, and annotated time-series plots."
      ],
      "metadata": {
        "id": "-rUERAU8v-Jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def task6_anomaly_detection(cleaned_df, device, contamination=0.01, save=True):\n",
        "    d = cleaned_df[cleaned_df[\"device_id\"] == device].set_index(\"timestamp\").sort_index()\n",
        "    sensors = [\"temperature_c\", \"humidity_pct\", \"vibration_level\", \"pressure_kpa\", \"light_lux\", \"gas_ppm\"]\n",
        "    X = d[sensors].fillna(method=\"ffill\").fillna(method=\"bfill\").values\n",
        "\n",
        "    # 1) Statistical: Z-score and IQR per sensor -> binary label if any sensor flagged\n",
        "    z_scores = np.abs(stats.zscore(X, nan_policy=\"omit\"))\n",
        "    z_anom = (z_scores > 3).any(axis=1)\n",
        "\n",
        "    # IQR\n",
        "    q1 = np.nanpercentile(X, 25, axis=0)\n",
        "    q3 = np.nanpercentile(X, 75, axis=0)\n",
        "    iqr = q3 - q1\n",
        "    lower = q1 - 1.5 * iqr\n",
        "    upper = q3 + 1.5 * iqr\n",
        "    iqr_anom = ((X < lower) | (X > upper)).any(axis=1)\n",
        "\n",
        "    # 2) Isolation Forest\n",
        "    iso = IsolationForest(contamination=contamination, random_state=42)\n",
        "    iso_labels = iso.fit_predict(X)\n",
        "    iso_anom = (iso_labels == -1)\n",
        "\n",
        "    # 3) LOF\n",
        "    lof = LocalOutlierFactor(n_neighbors=20, contamination=contamination)\n",
        "    lof_labels = lof.fit_predict(X)\n",
        "    lof_anom = (lof_labels == -1)\n",
        "\n",
        "    # 4) GMM probability: low-likelihood points flagged\n",
        "    gmm = GaussianMixture(n_components=2, random_state=42)\n",
        "    try:\n",
        "        gmm.fit(X)\n",
        "        scores = gmm.score_samples(X)  # log-likelihood\n",
        "        threshold = np.percentile(scores, 100 * contamination)\n",
        "        gmm_anom = scores < threshold\n",
        "    except Exception:\n",
        "        gmm_anom = np.zeros(len(X), dtype=bool)\n",
        "        scores = np.zeros(len(X))\n",
        "\n",
        "    # Consolidate: create DataFrame of anomaly flags\n",
        "    anom_df = pd.DataFrame(index=d.index)\n",
        "    anom_df[\"z_anom\"] = z_anom\n",
        "    anom_df[\"iqr_anom\"] = iqr_anom\n",
        "    anom_df[\"iso_anom\"] = iso_anom\n",
        "    anom_df[\"lof_anom\"] = lof_anom\n",
        "    anom_df[\"gmm_anom\"] = gmm_anom\n",
        "    # voting: anomaly if 2+ methods flag\n",
        "    anom_df[\"anomaly_vote\"] = anom_df.sum(axis=1) >= 2\n",
        "\n",
        "    if save:\n",
        "        anom_df.astype(int).to_csv(os.path.join(OUTPUT_DIR, f\"{device}_anomalies_methods.csv\"))\n",
        "        # quick plot\n",
        "        plt.figure(figsize=(14, 5))\n",
        "        plt.plot(d.index, d[\"temperature_c\"], label=\"temperature\")\n",
        "        plt.scatter(anom_df[anom_df[\"anomaly_vote\"]].index, d.loc[anom_df[anom_df[\"anomaly_vote\"]].index, \"temperature_c\"],\n",
        "                    color=\"red\", s=20, label=\"anomaly_vote\")\n",
        "        plt.title(f\"Anomaly votes for {device}\")\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, f\"{device}_anomaly_plot.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    # compute simple precision/recall if original labels exist\n",
        "    if \"anomaly_label\" in d.columns:\n",
        "        true = d[\"anomaly_label\"].fillna(0).astype(int).values\n",
        "        pred = anom_df[\"anomaly_vote\"].astype(int).values\n",
        "        # avoid zeros length\n",
        "        if len(true) == len(pred):\n",
        "            tp = ((true == 1) & (pred == 1)).sum()\n",
        "            fp = ((true == 0) & (pred == 1)).sum()\n",
        "            fn = ((true == 1) & (pred == 0)).sum()\n",
        "            precision = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
        "            recall = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
        "        else:\n",
        "            precision, recall = np.nan, np.nan\n",
        "    else:\n",
        "        precision, recall = np.nan, np.nan\n",
        "\n",
        "    return anom_df, {\"precision\": precision, \"recall\": recall}"
      ],
      "metadata": {
        "id": "g-LrpL8wv876"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 7: Sensor Health Scoring\n",
        "Objectives\n",
        "Evaluate each sensor’s reliability.\n",
        "Requirements\n",
        "1. Compute:\n",
        "- Stability score (variance)\n",
        "- Missing data score\n",
        "- Noise score (frequency-domain)\n",
        "- Anomaly rate\n",
        "2. Construct a composite index (0–100).\n",
        "3. Rank devices from most to least reliable."
      ],
      "metadata": {
        "id": "naBWIw4BwU-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def task7_sensor_health(cleaned_df):\n",
        "    devices = cleaned_df[\"device_id\"].unique()\n",
        "    rows = []\n",
        "    sensors = [\"temperature_c\", \"humidity_pct\", \"vibration_level\", \"pressure_kpa\", \"light_lux\", \"gas_ppm\"]\n",
        "\n",
        "    for device in devices:\n",
        "        d = cleaned_df[cleaned_df[\"device_id\"] == device]\n",
        "        total_count = len(d)\n",
        "        row = {\"device_id\": device}\n",
        "\n",
        "        # stability: inverse of variance aggregated across sensors\n",
        "        variances = {}\n",
        "        for s in sensors:\n",
        "            variances[s] = d[s].var(skipna=True)\n",
        "        stability_score = 100 - np.nanmean([min(variances[s], np.nanmedian(list(variances.values()))*10) for s in sensors])  # rough\n",
        "        # missing data score\n",
        "        missing_prop = d[sensors].isna().mean().mean()  # average missing proportion across sensors\n",
        "        missing_score = 100 * (1 - missing_prop)\n",
        "        # noise score (FFT energy variance as proxy)\n",
        "        noise_scores = []\n",
        "        for s in sensors:\n",
        "            series = d[s].dropna()\n",
        "            if len(series) > 10:\n",
        "                spec = np.abs(rfft(series - series.mean()))\n",
        "                noise_scores.append(np.var(spec))\n",
        "            else:\n",
        "                noise_scores.append(0.0)\n",
        "        noise_score = 100 - np.nanmean(noise_scores)/ (np.nanmax(noise_scores)+1e-9) * 100  # normalize-ish\n",
        "        # anomaly rate\n",
        "        anomaly_rate = d[\"anomaly_label\"].fillna(0).astype(int).mean() if \"anomaly_label\" in d.columns else 0\n",
        "        anomaly_score = 100 * (1 - anomaly_rate)\n",
        "\n",
        "        # composite index: weighted sum\n",
        "        composite = 0.35 * stability_score + 0.25 * missing_score + 0.2 * noise_score + 0.2 * anomaly_score\n",
        "        row.update({\n",
        "            \"stability_score\": float(np.clip(stability_score, 0, 100)),\n",
        "            \"missing_score\": float(np.clip(missing_score, 0, 100)),\n",
        "            \"noise_score\": float(np.clip(noise_score, 0, 100)),\n",
        "            \"anomaly_score\": float(np.clip(anomaly_score, 0, 100)),\n",
        "            \"composite_score\": float(np.clip(composite, 0, 100))\n",
        "        })\n",
        "        rows.append(row)\n",
        "\n",
        "    health_df = pd.DataFrame(rows).set_index(\"device_id\").sort_values(\"composite_score\", ascending=False)\n",
        "    health_df.to_csv(os.path.join(OUTPUT_DIR, \"device_health_scores.csv\"))\n",
        "    return health_df"
      ],
      "metadata": {
        "id": "m5_1kxL4wUVc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 8: Forecasting\n",
        "Objectives\n",
        "Forecast the next 24 hours of temperature for any device.\n",
        "Methods Allowed\n",
        "- ARIMA / SARIMA\n",
        "- Prophet\n",
        "- LSTM or GRU\n",
        "- Temporal CNN\n",
        "\n",
        "5\n",
        "\n",
        "Deliverables\n",
        "Forecast plot, MAE/RMSE metrics, and comparison of two models."
      ],
      "metadata": {
        "id": "pH4UNiG4wgNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def task8_forecast_temperature(cleaned_df, device, horizon_hours=24, freq_seconds=30, model_choice=\"ARIMA\"):\n",
        "    \"\"\"\n",
        "    Forecast next horizon_hours for temperature using ARIMA/SARIMAX.\n",
        "    Returns forecast series and error metrics (if holdout present).\n",
        "    \"\"\"\n",
        "    d = cleaned_df[cleaned_df[\"device_id\"] == device].set_index(\"timestamp\").sort_index()\n",
        "    ts = d[\"temperature_c\"].dropna()\n",
        "    if len(ts) < 50:\n",
        "        raise ValueError(\"Not enough data for forecasting\")\n",
        "\n",
        "    # resample to 30s if not strict\n",
        "    ts = ts.asfreq(f\"{freq_seconds}s\").fillna(method=\"ffill\")\n",
        "\n",
        "    # train/test split: last 24h as test if exists\n",
        "    samples_per_hour = int(3600 / freq_seconds)\n",
        "    horizon = horizon_hours * samples_per_hour\n",
        "    if len(ts) > horizon * 2:\n",
        "        train, test = ts[:-horizon], ts[-horizon:]\n",
        "    else:\n",
        "        train, test = ts[:-horizon], ts[-horizon:]  # may be small; still try\n",
        "\n",
        "    if model_choice == \"ARIMA\":\n",
        "        # simple ARIMA on differenced series with auto-order selection is not done here due to time.\n",
        "        model = ARIMA(train, order=(2,1,2))\n",
        "        fit = model.fit()\n",
        "        pred = fit.get_forecast(steps=horizon)\n",
        "        forecast = pred.predicted_mean\n",
        "    elif model_choice == \"SARIMAX\":\n",
        "        # seasonal example with weekly seasonality - example params; tune in practice\n",
        "        model = SARIMAX(train, order=(1,1,1), seasonal_order=(0,1,1, samples_per_hour*24//2 if samples_per_hour*24//2 < len(train) else 0))\n",
        "        fit = model.fit(disp=False)\n",
        "        forecast = fit.get_forecast(steps=horizon).predicted_mean\n",
        "    else:\n",
        "        raise ValueError(\"Only ARIMA/SARIMAX implemented in this example\")\n",
        "\n",
        "    # metrics\n",
        "    if len(test) == len(forecast):\n",
        "        mae = mean_absolute_error(test, forecast)\n",
        "        rmse = np.sqrt(mean_squared_error(test, forecast))\n",
        "    else:\n",
        "        mae = np.nan; rmse = np.nan\n",
        "\n",
        "    # plot\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(train.index[-(samples_per_hour*24):], train[-(samples_per_hour*24):], label=\"train (last 24h)\")\n",
        "    if len(test)>0:\n",
        "        plt.plot(test.index, test, label=\"test (actual)\")\n",
        "    plt.plot(pd.date_range(start=ts.index[-1] + pd.Timedelta(seconds=freq_seconds), periods=horizon, freq=f\"{freq_seconds}s\"), forecast, label=\"forecast\")\n",
        "    plt.title(f\"Temperature forecast for {device} — {model_choice}\")\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, f\"{device}_temperature_forecast_{model_choice}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    return forecast, {\"mae\": mae, \"rmse\": rmse}"
      ],
      "metadata": {
        "id": "2fVdxc_nwfpK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 9: Cross-Device Similarity Search\n",
        "Objectives\n",
        "Identify similar devices for maintenance and monitoring.\n",
        "Requirements\n",
        "1. Compute DTW similarity between devices.\n",
        "2. Use Euclidean distance on window-based features as an alternate metric.\n",
        "3. Visualize similarity network using NetworkX.\n",
        "Deliverables\n",
        "Top-5 nearest neighbors per device and a similarity graph."
      ],
      "metadata": {
        "id": "hpMd6EfHwtAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def task9_similarity_search(cleaned_df, metric=\"dtw\", device_sample=None, top_k=5):\n",
        "    devices = cleaned_df[\"device_id\"].unique()\n",
        "    if device_sample is None:\n",
        "        device_sample = devices[0]\n",
        "    # create short normalized time series per device for temperature (first N points)\n",
        "    ts_by_device = {}\n",
        "    N = 2000  # cap for distance computation\n",
        "    for device in devices:\n",
        "        s = cleaned_df[cleaned_df[\"device_id\"] == device].set_index(\"timestamp\")[\"temperature_c\"].dropna()\n",
        "        s = s.iloc[:N]\n",
        "        if len(s) < 10:\n",
        "            continue\n",
        "        s = (s - s.mean()) / (s.std() + 1e-9)\n",
        "        ts_by_device[device] = s.values\n",
        "\n",
        "    target = ts_by_device.get(device_sample)\n",
        "    if target is None:\n",
        "        raise ValueError(\"device_sample has no valid series\")\n",
        "\n",
        "    distances = []\n",
        "    for d, series in ts_by_device.items():\n",
        "        if d == device_sample:\n",
        "            continue\n",
        "        if metric == \"dtw\" and has_fastdtw:\n",
        "            dist, _ = fastdtw(target, series, dist=euclidean)\n",
        "        else:\n",
        "            # fallback: Euclidean distance on truncated or padded series\n",
        "            L = min(len(target), len(series))\n",
        "            dist = np.linalg.norm(target[:L] - series[:L])\n",
        "        distances.append((d, dist))\n",
        "    distances = sorted(distances, key=lambda x: x[1])\n",
        "    topk = distances[:top_k]\n",
        "\n",
        "    # build network graph\n",
        "    G = nx.Graph()\n",
        "    G.add_node(device_sample)\n",
        "    for d, dist in topk:\n",
        "        G.add_node(d)\n",
        "        G.add_edge(device_sample, d, weight=float(dist))\n",
        "\n",
        "    # save graph in GML and plot\n",
        "    nx.write_gml(G, os.path.join(OUTPUT_DIR, f\"{device_sample}_similarity_graph.gml\"))\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    pos = nx.spring_layout(G, seed=42)\n",
        "    nx.draw(G, pos, with_labels=True, node_size=500, font_size=8)\n",
        "    edge_labels = nx.get_edge_attributes(G, \"weight\")\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=7)\n",
        "    plt.title(f\"Top-{top_k} similar devices to {device_sample} using {metric}\")\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, f\"{device_sample}_similarity_network_{metric}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    return topk, G"
      ],
      "metadata": {
        "id": "AVD1OULqwpvN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 10: Final Report\n",
        "A professional report including:\n",
        "- Cleaning and preprocessing summary\n",
        "- Methods used\n",
        "- Key insights (clusters, anomalies, reliability, forecasting)\n",
        "- Business/operational interpretations\n",
        "- Appendices with plots and tables"
      ],
      "metadata": {
        "id": "ySd1TwZIwy8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_report():\n",
        "    # This is a placeholder function that collects artifacts and writes a simple index\n",
        "    artifacts = [f for f in os.listdir(OUTPUT_DIR) if not f.startswith(\".\")]\n",
        "    with open(os.path.join(OUTPUT_DIR, \"report_index.txt\"), \"w\") as f:\n",
        "        f.write(\"Artifacts produced:\\n\\n\")\n",
        "        for a in artifacts:\n",
        "            f.write(a + \"\\n\")\n",
        "    print(\"Report index generated at:\", os.path.join(OUTPUT_DIR, \"report_index.txt\"))\n",
        "\n",
        "# -------------------------\n",
        "# Example orchestration / quick-run\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    df = load_data(DATA_PATH)\n",
        "    print(\"Loaded rows:\", len(df))\n",
        "\n",
        "    # Task 1\n",
        "    missing_df, cleaned_df = task1_integrity_and_cleaning(df)\n",
        "    print(\"Missing summary saved. Cleaned saved.\")\n",
        "\n",
        "    # Task 2 (example on first device)\n",
        "    sample_device = cleaned_df[\"device_id\"].unique()[0]\n",
        "    t2 = task2_denoise_and_decompose(cleaned_df, device=sample_device)\n",
        "    print(\"Denoising & decomposition done for\", sample_device)\n",
        "\n",
        "    # Task 3\n",
        "    profiles_df, best_k, best_score = task3_profile_and_cluster(cleaned_df)\n",
        "    print(\"Clustering done: k=\", best_k, \"silhouette=\", best_score)\n",
        "\n",
        "    # Task 4 (rolling features for sample device)\n",
        "    rolling_feats = task4_rolling_features(cleaned_df, sample_device)\n",
        "    print(\"Rolling features saved for\", sample_device)\n",
        "\n",
        "    # Task 5\n",
        "    pearson, spearman, xc = task5_correlations(cleaned_df)\n",
        "    print(\"Correlation images saved. Cross-corr sample:\", xc)\n",
        "\n",
        "    # Task 6 (anomalies sample device)\n",
        "    anom_df, metrics = task6_anomaly_detection(cleaned_df, sample_device)\n",
        "    print(\"Anomaly detection done. precision/recall:\", metrics)\n",
        "\n",
        "    # Task 7\n",
        "    health = task7_sensor_health(cleaned_df)\n",
        "    print(\"Health scoring done. Top devices:\\n\", health.head())\n",
        "\n",
        "    # Task 8 (forecast for sample device)\n",
        "    try:\n",
        "        forecast, fmetrics = task8_forecast_temperature(cleaned_df, sample_device, model_choice=\"ARIMA\")\n",
        "        print(\"Forecast done. MAE/RMSE:\", fmetrics)\n",
        "    except Exception as e:\n",
        "        print(\"Forecast error:\", e)\n",
        "\n",
        "    # Task 9\n",
        "    topk, G = task9_similarity_search(cleaned_df, metric=\"dtw\" if has_fastdtw else \"euclidean\", device_sample=sample_device)\n",
        "    print(\"Top similar devices:\", topk)\n",
        "\n",
        "    # Task 10\n",
        "    generate_report()\n",
        "    print(\"Pipeline finished. Check the 'output' folder for artifacts.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfKZSvYCwyTq",
        "outputId": "78400d89-f747-4cf2-9c5c-f5069053a89a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded rows: 20000\n",
            "Missing summary saved. Cleaned saved.\n",
            "Denoising & decomposition done for DEV_001\n",
            "Clustering done: k= 2 silhouette= 0.10620563116123012\n",
            "Rolling features saved for DEV_001\n",
            "Correlation images saved. Cross-corr sample: {'DEV_001_vs_DEV_002': {'lag': 5075, 'max_corr': 4665.065089337741}, 'DEV_002_vs_DEV_003': {'lag': 8511, 'max_corr': 2975.467108382855}, 'DEV_003_vs_DEV_004': {'lag': -8984, 'max_corr': 3766.1811014270597}}\n",
            "Anomaly detection done. precision/recall: {'precision': np.float64(0.005097706032285472), 'recall': np.float64(1.0)}\n",
            "Health scoring done. Top devices:\n",
            "            stability_score  missing_score  noise_score  anomaly_score  \\\n",
            "device_id                                                               \n",
            "DEV_001                0.0      84.120438    82.609459      99.969823   \n",
            "DEV_002                0.0      84.140178    82.291499      99.959700   \n",
            "DEV_003                0.0      84.169803    82.354740      99.969887   \n",
            "DEV_004                0.0      84.126266    82.538859      99.964796   \n",
            "DEV_005                0.0      84.043160    82.243417      99.969795   \n",
            "\n",
            "           composite_score  \n",
            "device_id                   \n",
            "DEV_001                0.0  \n",
            "DEV_002                0.0  \n",
            "DEV_003                0.0  \n",
            "DEV_004                0.0  \n",
            "DEV_005                0.0  \n",
            "Forecast done. MAE/RMSE: {'mae': 2.4370512194335987, 'rmse': np.float64(3.2910957619612375)}\n",
            "Top similar devices: [('DEV_031', np.float64(42.92763177170059)), ('DEV_043', np.float64(44.950788444433556)), ('DEV_078', np.float64(46.767154273290146)), ('DEV_028', np.float64(48.85036460517397)), ('DEV_066', np.float64(49.587164319608306))]\n",
            "Report index generated at: output/report_index.txt\n",
            "Pipeline finished. Check the 'output' folder for artifacts.\n"
          ]
        }
      ]
    }
  ]
}